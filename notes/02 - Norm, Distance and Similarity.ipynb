{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norm\n",
    "\n",
    "\n",
    "* Norm is a function that returns the length of a vector. \n",
    "* A **normed vector** means a vector has length and is not negative. i.e. $||x|| > 0$\n",
    "* Given a vector space $V$ where $\\boldsymbol{u}, \\boldsymbol{v} \\in V$, the norm of a vector $p$ has three properties:\n",
    "    1. $p(\\boldsymbol{u} + \\boldsymbol{v}) <= p(\\boldsymbol{u}) + p(\\boldsymbol{v})$ (Triangle Inequality)\n",
    "    2. $p(a\\boldsymbol{v}) = |a| p(\\boldsymbol{v})$ (Length has no direction)\n",
    "    3. if $p(\\boldsymbol{v}) = 0$ then $V$ is a zero vector (Zero vector has zero length)\n",
    "* The length of a vector can be think of as the distance between the origin and the vector's head.\n",
    "\n",
    "\n",
    "## $p$ norm\n",
    "\n",
    "Also known as $L_p$ norm:\n",
    "\n",
    "### $||x||_p := (\\sum\\limits_{i}^{n}{|x_i|^p})^\\frac{1}{p}$\n",
    "\n",
    "## $L_1$ norm\n",
    "\n",
    "### $||x||_1 := (\\sum\\limits_{i}^{n}{|x_i|})$\n",
    "\n",
    "## $L_2$ norm\n",
    "\n",
    "### $||x||_2 := (\\sum\\limits_{i}^{n}{|x_i|^2})^\\frac{1}{2}$\n",
    "### $||x||_2 := \\sqrt{x_1^2 + x_2^2 + ... + x_n^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "### $L(\\theta) = - \\sum\\limits_{i} y_i \\log(\\hat{y}) + \\lambda R(\\theta)$\n",
    "\n",
    "where $\\theta$ is a vector of the parameters.\n",
    "\n",
    "### $L_1$ Regularization: $R(\\theta) = ||\\theta||_1$ \n",
    "\n",
    "- Parameters become sparse\n",
    "- Less sensetive to outliers\n",
    "- Has multiple solution\n",
    "\n",
    "### $L_2$ Regularization: $R(\\theta) = ||\\theta||_2$ \n",
    "sometimes,$||\\theta||_2^2$ is used.\n",
    "\n",
    "- Global minimum\n",
    "- Experience shows $L_2$ norm usually outperforms $L_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance\n",
    "\n",
    "## Minkowski\n",
    "\n",
    "### $D(X, Y) = (\\sum\\limits_{i}^{n} |x_i - y_i|^p)^\\frac{1}{p}$\n",
    "\n",
    "## Manhattan\n",
    "\n",
    "### $D(X, Y) = (\\sum\\limits_{i}^{n} |x_i - y_i|)$\n",
    "\n",
    "\n",
    "## Euclidean\n",
    "\n",
    "### $D(X, Y) = \\sqrt{\\sum\\limits_{i}^{n} (x_i - y_i)^2}$\n",
    "\n",
    "\n",
    "## Chebyshev\n",
    "\n",
    "### $D(X, Y) = \\lim\\limits_{p \\rightarrow \\infty}(\\sum\\limits_{i}^{n} |x_i - y_i|^p)^\\frac{1}{p}$\n",
    "\n",
    "### $D(X, Y) = \\max\\limits_{i}{(|x_i - y_i|)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity\n",
    "\n",
    "$-1 \\times D(X, Y)$ can be viewed as similarity as well. e.g. $S = - \\sqrt{\\sum\\limits_{i}^{n} (x_i - y_i)^2}$ is the reverse of the euclidean distance, where $-0$ is considered most similar. In Affinity Propagation, this kind of similarity is used.\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "$a \\cdot b = ||a|| ||b|| cos(\\theta)$\n",
    "\n",
    "\n",
    "## $cos(\\theta) = \\frac{A \\cdot B}{||A|| ||B||}$\n",
    "\n",
    "Q: Why use cosine similarity?\n",
    "\n",
    "Consider the case of tf-idf representation for retreving document, document $A$ and $B$ can be using similar words but repeated different times, i.e. $A = [1, 1, 0, 0, 1]$, $B = [10, 10, 0.1, 0.1, 10]$. In this case, distance measured by e.g. euclidean distance will be very large, however, the cosine similarity properly reflects that two documents are actually similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tgmle-dev]",
   "language": "python",
   "name": "conda-env-tgmle-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
